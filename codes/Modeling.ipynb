{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-54-153.ec2.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7f670dd5f8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf = spark.read\\\n",
    "  .format('csv')\\\n",
    "  .option('header', 'true')\\\n",
    "  .option('inferSchema', 'true')\\\n",
    "  .load('user_log.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- cat_id: integer (nullable = true)\n",
      " |-- merchant_id: integer (nullable = true)\n",
      " |-- brand_id: double (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- action: integer (nullable = true)\n",
      " |-- age_range: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mydf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import the packages and methods we need\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.feature import Binarizer\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=404803, merchant_id=1535, age_range=6, action=2, gender='female', count=1),\n",
       " Row(user_id=348498, merchant_id=1724, age_range=0, action=2, gender='female', count=1),\n",
       " Row(user_id=404803, merchant_id=1535, age_range=4, action=0, gender='female', count=1),\n",
       " Row(user_id=209797, merchant_id=461, age_range=3, action=3, gender='female', count=1),\n",
       " Row(user_id=185159, merchant_id=3634, age_range=5, action=0, gender='female', count=1),\n",
       " Row(user_id=179739, merchant_id=1805, age_range=5, action=2, gender='female', count=1),\n",
       " Row(user_id=185159, merchant_id=606, age_range=6, action=0, gender='female', count=1),\n",
       " Row(user_id=313749, merchant_id=2206, age_range=6, action=0, gender='female', count=1),\n",
       " Row(user_id=185159, merchant_id=831, age_range=1, action=0, gender='female', count=1),\n",
       " Row(user_id=350592, merchant_id=4330, age_range=1, action=0, gender='female', count=1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydf.groupBy('user_id','merchant_id','age_range','action','gender').count().orderBy('count').take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = mydf.groupBy('user_id','merchant_id','age_range','action','gender').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=185858, merchant_id=1636, age_range=4, action=0, gender='female', count=1),\n",
       " Row(user_id=375886, merchant_id=3716, age_range=3, action=0, gender='female', count=1),\n",
       " Row(user_id=375886, merchant_id=4047, age_range=4, action=0, gender='female', count=1),\n",
       " Row(user_id=253184, merchant_id=1428, age_range=5, action=0, gender='female', count=1),\n",
       " Row(user_id=308768, merchant_id=759, age_range=1, action=0, gender='female', count=1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- merchant_id: integer (nullable = true)\n",
      " |-- age_range: integer (nullable = true)\n",
      " |-- action: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## since we are gonna use Logistic regression, we need to create a new column called if_return to identify whether or not\n",
    "## a customer repeatedly buy items from a certain merchant\n",
    "binarizer = Binarizer(threshold=1, inputCol=\"label\", outputCol=\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = newdf.withColumn(\"label\", newdf[\"count\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = binarizer.transform(newdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- merchant_id: integer (nullable = true)\n",
      " |-- age_range: integer (nullable = true)\n",
      " |-- action: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      " |-- label: double (nullable = false)\n",
      " |-- labels: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Split into training and testing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training records: 29214817\n",
      "Number of testing records : 5158740\n"
     ]
    }
   ],
   "source": [
    "splitted_data = newdf.randomSplit([0.85, 0.15], 2333)\n",
    "train_data = splitted_data[0]\n",
    "test_data = splitted_data[1]\n",
    "\n",
    "print(\"Number of training records: \" + str(train_data.count()))\n",
    "print(\"Number of testing records : \" + str(test_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***train logistic model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexer_age_range = StringIndexer(inputCol=\"age_range\", outputCol=\"age_range_id\")\n",
    "stringIndexer_gender = StringIndexer(inputCol=\"gender\", outputCol=\"gender_id\")\n",
    "stringIndexer_action = StringIndexer(inputCol=\"action\", outputCol=\"action_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_fit = StringIndexer(inputCol=\"labels\", outputCol=\"labels_id\").fit(newdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_col = [\n",
    "            'age_range_id',\n",
    "            'gender_id',\n",
    "            \"age_range_id\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In the following step, create a feature vector by using the vectorAssembler method\n",
    "vectorAssembler_features = VectorAssembler(\n",
    "    inputCols=input_col, \n",
    "    outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define estimator, use LogisticRegression estimator\n",
    "lr = LogisticRegression(featuresCol='features', labelCol= 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build the pipeline\n",
    "pipeline_lr = Pipeline(stages=[\n",
    "                                stringIndexer_action,\n",
    "                                stringIndexer_age_range,\n",
    "                                stringIndexer_gender,\n",
    "                                vectorAssembler_features,\n",
    "                                lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train model\n",
    "model_lr = pipeline_lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prediction\n",
    "predictions = model_lr.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_label = predictions.select('prediction','labels').rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationMetrics(prediction_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.547\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluator.areaUnderROC\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"labels\", featuresCol=\"features\", numTrees=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build the pipeline\n",
    "pipeline_rf = Pipeline(stages=[\n",
    "                                stringIndexer_action,\n",
    "                                stringIndexer_age_range,\n",
    "                                stringIndexer_gender,\n",
    "                                vectorAssembler_features,\n",
    "                                rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train model\n",
    "model_rf = pipeline_rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prediction\n",
    "predictions = model_rf.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_label = predictions.select('prediction','labels').rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationMetrics(prediction_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.561\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluator.areaUnderROC\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
